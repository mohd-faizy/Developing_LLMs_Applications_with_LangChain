{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e479b61",
   "metadata": {},
   "source": [
    "# **üî∑üî∑Introduction to Graph RAGüî∑üî∑**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a46733",
   "metadata": {},
   "source": [
    "## **‚≠ê01: From vectors to graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5f203",
   "metadata": {},
   "source": [
    "### **‚≠ïVector RAG Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dbce3",
   "metadata": {},
   "source": [
    "![img_1](https://github.com/mohd-faizy/Developing_LLMs_Applications_with_LangChain/blob/main/_img/0601.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d333e",
   "metadata": {},
   "source": [
    "Vector-based RAG (Retrieval-Augmented Generation) systems combine **vector similarity search** with **generative models** (like GPT) to improve response accuracy by fetching relevant information from a knowledge base. While powerful, **Vector RAG has several limitations**, especially in real-world applications.\n",
    "\n",
    "Here‚Äôs a breakdown of the key limitations:\n",
    "\n",
    "\n",
    "\n",
    "#### **1. Semantic Drift or Irrelevant Retrieval**\n",
    "\n",
    "**Problem:** Vector search retrieves documents based on *semantic similarity*, not factual accuracy or context relevance.\n",
    "\n",
    "**Example:**\n",
    "Suppose a user asks:\n",
    "\n",
    "> \"What are the side effects of metformin?\"\n",
    "\n",
    "The system may retrieve chunks containing:\n",
    "\n",
    "* \"The benefits of metformin in managing blood sugar\"\n",
    "* \"Metformin's role in preventing diabetes complications\"\n",
    "\n",
    "**Issue:** These are related **semantically**, but they may **not directly mention side effects**. The model may then hallucinate or guess side effects, leading to unreliable answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Loss of Granularity**\n",
    "\n",
    "**Problem:** When documents are chunked too large or too small, RAG systems struggle.\n",
    "\n",
    "**Example:**\n",
    "A 20-page medical paper is chunked into 1-page chunks. The user's query:\n",
    "\n",
    "> \"What did the clinical trial say about metformin and weight loss?\"\n",
    "\n",
    "If the specific trial is mentioned briefly in just one paragraph, vector search may **miss it** if:\n",
    "\n",
    "* Chunk is too broad and dilutes relevance\n",
    "* Embedding doesn't capture that fine-grained detail\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **3. No True Understanding of Context**\n",
    "\n",
    "**Problem:** Vector similarity is context-agnostic ‚Äî it doesn‚Äôt consider previous turns in a conversation unless explicitly added.\n",
    "\n",
    "**Example:**\n",
    "**Conversation:**\n",
    "\n",
    "* User: \"Tell me about Python decorators.\"\n",
    "* Then: \"What about their use in Flask?\"\n",
    "\n",
    "Vector search may **not know what \"their\" refers to**, unless conversation history is added to the query, which RAG doesn‚Äôt do natively.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **4. Stale or Outdated Knowledge**\n",
    "\n",
    "**Problem:** Vector RAG systems are only as up-to-date as their vector store.\n",
    "\n",
    "**Example:**\n",
    "User asks:\n",
    "\n",
    "> \"What's the latest security patch in Log4j?\"\n",
    "\n",
    "If the vector DB was last updated in 2022, it might **miss 2024 disclosures**, leading to outdated or dangerous information.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **5. Embedding Quality and Limitations**\n",
    "\n",
    "**Problem:** Embeddings might not capture important domain-specific nuances.\n",
    "\n",
    "**Example:**\n",
    "For a legal query like:\n",
    "\n",
    "> \"What precedent did the judge rely on in Roe v. Wade?\"\n",
    "\n",
    "Legal documents may contain dense citations and context-specific phrases. General-purpose embeddings may not **capture the key precedent accurately**, resulting in **weak or unrelated chunks** being retrieved.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **6. Long Documents, Poor Chunking**\n",
    "\n",
    "**Problem:** Poor chunking strategies lead to missed context or cutoff info.\n",
    "\n",
    "**Example:**\n",
    "In a user manual, the definition of \"safe operating temperature\" might be split across two chunks. The RAG system may retrieve only half of the definition, leading the model to guess or produce partial info.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **7. Lack of Ranking or Reranking**\n",
    "\n",
    "**Problem:** Vector RAG might retrieve the *top k* most similar documents but **not the most useful or trustworthy** ones.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> \"What causes high CPU usage in Kubernetes?\"\n",
    "\n",
    "The top vector match might describe CPU metrics collection, while a lower-ranked chunk (ignored) contains the **actual explanation** for common CPU issues (e.g., liveness probes, throttling, etc.).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Limitation                       | Description                                         | Example                               |\n",
    "| -------------------------------- | --------------------------------------------------- | ------------------------------------- |\n",
    "| `Semantic Drift`                 | Retrieval is related but not precise                | Side effects vs benefits of metformin |\n",
    "| `Granularity Issues`             | Too broad/narrow chunks miss details                | Specific paragraph in long PDF missed |\n",
    "| `Lack of Conversational Context` | Fails to understand pronouns or dialogue continuity | \"What about their use?\" ambiguity     |\n",
    "| `Outdated Knowledge`             | Vector store isn't real-time                        | Security patch from 2024 missing      |\n",
    "| `Embedding Limitations`          | General embeddings miss domain-specific meaning     | Legal or medical terms                |\n",
    "| `Poor Chunking`                  | Important info split across boundaries              | Definition split between chunks       |\n",
    "| `No Reranking or Trust Score`    | Most relevant docs may be ranked lower              | Actual CPU issue not retrieved        |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51178dc6",
   "metadata": {},
   "source": [
    "### **‚≠ïGraph Databases ‚Äì Nodes and Edges**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67de2a9",
   "metadata": {},
   "source": [
    "![img_2](https://github.com/mohd-faizy/Developing_LLMs_Applications_with_LangChain/blob/main/_img/0602.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e6e44",
   "metadata": {},
   "source": [
    "Graph Databases ‚Äì Nodes and Edges in the context of Graph RAG (Retrieval-Augmented Generation) \n",
    "\n",
    "#### üîç **Graph Databases ‚Äì Simple Explanation**\n",
    "\n",
    "* **Graph databases** store data like a web of connected dots (like a mind map).\n",
    "* These dots are called **Nodes** (represent real things like a person, model, or idea).\n",
    "* The lines between them are called **Edges** (show how those things are related).\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© **Components**\n",
    "\n",
    "* **Nodes = Things (entities)**\n",
    "  Example:\n",
    "\n",
    "  * `Gpt-4` (a model)\n",
    "  * `OpenAI` (a company)\n",
    "\n",
    "* **Edges = Relationships between things**\n",
    "  Example:\n",
    "\n",
    "  * `Gpt-4` ‚ûù `DEVELOPED_BY` ‚ûù `OpenAI`\n",
    "  * `ChatGPT` ‚ûù `MENTIONS` ‚ûù `Gpt-4`\n",
    "  * `AutoGPT` ‚ûù `BASED_ON` ‚ûù `Gpt-4`\n",
    "\n",
    "---\n",
    "\n",
    "#### ü§ñ **How it's used in Graph RAG**\n",
    "\n",
    "* You can ask a question like:\n",
    "  *\"Who developed Gpt-4?\"*\n",
    "\n",
    "* The system follows the edge:\n",
    "\n",
    "  * Start at node `Gpt-4`\n",
    "  * Follow `DEVELOPED_BY` ‚ûù reach node `OpenAI`\n",
    "  * ‚úÖ Answer: `OpenAI`\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ **Why Graphs are Useful**\n",
    "\n",
    "* Makes it easier to **trace relationships** step-by-step.\n",
    "* Helps LLMs **reason clearly** and **give explainable answers**.\n",
    "* Good for **complex questions** involving multiple steps or layers of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdf515",
   "metadata": {},
   "source": [
    "### **‚≠ïLoading and Chunking Wikipedia Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc71a27",
   "metadata": {},
   "source": [
    "#### üîÑ **How Unstructured Text is Converted into Graph Data**\n",
    "\n",
    "* üìö **Start with Wikipedia** as the knowledge source (it's full of useful info).\n",
    "* üîç Use **WikipediaLoader** to get articles related to your question.\n",
    "* ‚úÇÔ∏è Split the article into smaller parts (chunks) using **TokenTextSplitter**\n",
    "*  ‚û§ This helps the model handle the text properly (due to input size limits).\n",
    "* üß† These smaller chunks are then used to **build a graph**, making search and reasoning easier.\n",
    "\n",
    "---\n",
    "\n",
    "#### üìù **Additional Points**\n",
    "\n",
    "* ‚úÖ **Good chunking is important** ‚Äì It keeps the meaning of the text clear when reasoning later.\n",
    "* üîß Graphs are created from these chunks using either:\n",
    "\n",
    "  * **LLMs (smart models)** or\n",
    "  * **Rule-based systems** (predefined logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f595e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\n",
      "The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding' metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import WikipediaLoader from langchain_community\n",
    "# This class allows us to programmatically fetch Wikipedia articles based on a query.\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Step 2: Import TokenTextSplitter from langchain_text_splitters\n",
    "# TokenTextSplitter breaks down large documents into smaller, manageable chunks based on token counts.\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "\n",
    "# Step 3: Load Wikipedia documents related to a specific topic (e.g., \"large language model\")\n",
    "# This returns a list of Document objects, each containing content and metadata like title and source.\n",
    "raw_documents = WikipediaLoader(query=\"large language model\").load()\n",
    "\n",
    "# Step 4: Initialize a TokenTextSplitter\n",
    "# chunk_size = 100 ‚Üí each text chunk will be 100 tokens long\n",
    "# chunk_overlap = 20 ‚Üí 20 tokens from the previous chunk will be included in the next to maintain context\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# Step 5: Split the raw documents into smaller chunks\n",
    "# Splitting only the first 3 documents from the Wikipedia query result for simplicity/performance\n",
    "# This helps make downstream processing (e.g., embeddings, retrieval) more efficient and accurate\n",
    "documents = text_splitter.split_documents(raw_documents[:3])\n",
    "\n",
    "# Step 6: Print the first chunk to inspect its structure\n",
    "# Useful to verify what kind of data (page content + metadata) we are working with\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d444258",
   "metadata": {},
   "source": [
    "```\n",
    "page_content='A large language model (LLM) is a computational model capable of...'  \n",
    "metadata={'title': 'Large language model', \n",
    "          'summary': \"A large language model (LLM) is...\", \n",
    "          'source': 'https://en.wikipedia.org/wiki/Large_language_model'} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d90d44",
   "metadata": {},
   "source": [
    "### **‚≠ïConverting Text to Graph Structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dbbaebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Llm', type='Language model', properties={}), Node(id='Gpt', type='Transformer', properties={}), Node(id='Chatgpt', type='Chatbot', properties={}), Node(id='Gemini', type='Chatbot', properties={}), Node(id='Claude', type='Chatbot', properties={})], relationships=[Relationship(source=Node(id='Llm', type='Language model', properties={}), target=Node(id='Gpt', type='Transformer', properties={}), type='INCLUDES', properties={}), Relationship(source=Node(id='Gpt', type='Transformer', properties={}), target=Node(id='Chatgpt', type='Chatbot', properties={}), type='USED_IN', properties={}), Relationship(source=Node(id='Gpt', type='Transformer', properties={}), target=Node(id='Gemini', type='Chatbot', properties={}), type='USED_IN', properties={}), Relationship(source=Node(id='Gpt', type='Transformer', properties={}), target=Node(id='Claude', type='Chatbot', properties={}), type='USED_IN', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content='A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding')), GraphDocument(nodes=[Node(id='Language Models', type='Concept', properties={}), Node(id='Transformer-Based Models', type='Model', properties={}), Node(id='Ibm', type='Organization', properties={}), Node(id='Statistical Models', type='Model', properties={})], relationships=[Relationship(source=Node(id='Language Models', type='Concept', properties={}), target=Node(id='Transformer-Based Models', type='Model', properties={}), type='EVOLVED_INTO', properties={}), Relationship(source=Node(id='Ibm', type='Organization', properties={}), target=Node(id='Statistical Models', type='Model', properties={}), type='DEVELOPED', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=\" be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\\n\\n\\n== History ==\\n\\nBefore the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques\")), GraphDocument(nodes=[Node(id='Ibm', type='Company', properties={}), Node(id='Statistical Models', type='Models', properties={}), Node(id='Word Alignment Techniques', type='Techniques', properties={}), Node(id='Machine Translation', type='Technology', properties={}), Node(id='Corpus-Based Language Modeling', type='Modeling', properties={}), Node(id='Smoothed N-Gram Model', type='Model', properties={}), Node(id='Kneser-Ney Smoothing', type='Technique', properties={}), Node(id='Benchmark Tests', type='Tests', properties={}), Node(id='Internet', type='Technology', properties={})], relationships=[Relationship(source=Node(id='Ibm', type='Company', properties={}), target=Node(id='Statistical Models', type='Models', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Statistical Models', type='Models', properties={}), target=Node(id='Word Alignment Techniques', type='Techniques', properties={}), type='PIONEER', properties={}), Relationship(source=Node(id='Word Alignment Techniques', type='Techniques', properties={}), target=Node(id='Machine Translation', type='Technology', properties={}), type='CONTRIBUTION', properties={}), Relationship(source=Node(id='Machine Translation', type='Technology', properties={}), target=Node(id='Corpus-Based Language Modeling', type='Modeling', properties={}), type='FOUNDATION', properties={}), Relationship(source=Node(id='Corpus-Based Language Modeling', type='Modeling', properties={}), target=Node(id='Smoothed N-Gram Model', type='Model', properties={}), type='INFLUENCE', properties={}), Relationship(source=Node(id='Smoothed N-Gram Model', type='Model', properties={}), target=Node(id='Kneser-Ney Smoothing', type='Technique', properties={}), type='EMPLOYMENT', properties={}), Relationship(source=Node(id='Smoothed N-Gram Model', type='Model', properties={}), target=Node(id='Benchmark Tests', type='Tests', properties={}), type='EVALUATION', properties={}), Relationship(source=Node(id='Internet', type='Technology', properties={}), target=Node(id='Benchmark Tests', type='Tests', properties={}), type='FACILITATION', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=\" data constraints of their time. In the early 1990s, IBM's statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. A smoothed n-gram model in 2001, such as those employing Kneser-Ney smoothing, trained on 300 million words achieved state-of-the-art perplexity on benchmark tests at the time. During the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the\")), GraphDocument(nodes=[Node(id='2000S', type='Time period', properties={}), Node(id='Internet', type='Technology', properties={}), Node(id='Researchers', type='Person', properties={}), Node(id='Text Datasets', type='Data', properties={}), Node(id='Statistical Language Models', type='Model', properties={}), Node(id='Deep Neural Networks', type='Model', properties={}), Node(id='Image Classification', type='Task', properties={}), Node(id='Word Embeddings', type='Technique', properties={}), Node(id='Word2Vec', type='Model', properties={}), Node(id='Mikolov', type='Person', properties={}), Node(id='Sequence-To-Sequence Models', type='Model', properties={}), Node(id='Lstm', type='Model', properties={})], relationships=[Relationship(source=Node(id='Researchers', type='Person', properties={}), target=Node(id='Text Datasets', type='Data', properties={}), type='COMPILED', properties={}), Relationship(source=Node(id='Text Datasets', type='Data', properties={}), target=Node(id='Statistical Language Models', type='Model', properties={}), type='TRAINED', properties={}), Relationship(source=Node(id='Deep Neural Networks', type='Model', properties={}), target=Node(id='Image Classification', type='Task', properties={}), type='APPLIED', properties={}), Relationship(source=Node(id='Word Embeddings', type='Technique', properties={}), target=Node(id='Word2Vec', type='Model', properties={}), type='EXAMPLE', properties={}), Relationship(source=Node(id='Mikolov', type='Person', properties={}), target=Node(id='Word2Vec', type='Model', properties={}), type='DEVELOPED', properties={}), Relationship(source=Node(id='Sequence-To-Sequence Models', type='Model', properties={}), target=Node(id='Lstm', type='Model', properties={}), type='USES', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (\"web as corpus\") to train statistical language models.\\n\\nFollowing the breakthrough of deep neural networks in image classification around 2012, similar architectures were adapted for language tasks. This shift was marked by the development of word embeddings (eg, Word2Vec by Mikolov in 2013) and sequence-to-sequence (seq2seq) models using LSTM. In 2016')), GraphDocument(nodes=[Node(id='Google', type='Organization', properties={}), Node(id='Lstm', type='Technique', properties={}), Node(id='Neural Machine Translation', type='Technology', properties={}), Node(id='Transformer Architecture', type='Model', properties={}), Node(id='Neurips', type='Conference', properties={}), Node(id='Google Researchers', type='Person', properties={})], relationships=[Relationship(source=Node(id='Google', type='Organization', properties={}), target=Node(id='Neural Machine Translation', type='Technology', properties={}), type='TRANSITIONED_TO', properties={}), Relationship(source=Node(id='Lstm', type='Technique', properties={}), target=Node(id='Neural Machine Translation', type='Technology', properties={}), type='USED_IN', properties={}), Relationship(source=Node(id='Google Researchers', type='Person', properties={}), target=Node(id='Transformer Architecture', type='Model', properties={}), type='INTRODUCED', properties={}), Relationship(source=Node(id='Neurips', type='Conference', properties={}), target=Node(id='Transformer Architecture', type='Model', properties={}), type='PRESENTED_AT', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=') and sequence-to-sequence (seq2seq) models using LSTM. In 2016, Google transitioned its translation service to neural machine translation (NMT), replacing statistical phrase-based models with deep recurrent neural networks. These early NMT systems used LSTM-based encoder-decoder architectures, as they preceded the invention of transformers.\\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\".')), GraphDocument(nodes=[Node(id='Google Researchers', type='Researchers', properties={}), Node(id='Transformer Architecture', type='Architecture', properties={}), Node(id='Attention Is All You Need', type='Paper', properties={}), Node(id='Seq2Seq Technology', type='Technology', properties={}), Node(id='Bahdanau Et Al.', type='Researchers', properties={}), Node(id='Attention Mechanism', type='Mechanism', properties={}), Node(id='Bert', type='Model', properties={}), Node(id='Encoder Blocks', type='Blocks', properties={}), Node(id='Decoder Blocks', type='Blocks', properties={})], relationships=[Relationship(source=Node(id='Google Researchers', type='Researchers', properties={}), target=Node(id='Transformer Architecture', type='Architecture', properties={}), type='INTRODUCED', properties={}), Relationship(source=Node(id='Transformer Architecture', type='Architecture', properties={}), target=Node(id='Attention Is All You Need', type='Paper', properties={}), type='PUBLISHED_IN', properties={}), Relationship(source=Node(id='Attention Is All You Need', type='Paper', properties={}), target=Node(id='Seq2Seq Technology', type='Technology', properties={}), type='IMPROVED_UPON', properties={}), Relationship(source=Node(id='Bahdanau Et Al.', type='Researchers', properties={}), target=Node(id='Attention Mechanism', type='Mechanism', properties={}), type='DEVELOPED', properties={}), Relationship(source=Node(id='Transformer Architecture', type='Architecture', properties={}), target=Node(id='Bert', type='Model', properties={}), type='INSPIRED', properties={}), Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Encoder Blocks', type='Blocks', properties={}), type='COMPOSED_OF', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper\\'s goal was to improve upon 2014 seq2seq technology, and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of B')), GraphDocument(nodes=[Node(id='Bert', type='Model', properties={}), Node(id='Gpt-1', type='Model', properties={}), Node(id='Gpt-2', type='Model', properties={}), Node(id='Decoder-Only Models', type='Models', properties={}), Node(id='Openai', type='Organization', properties={}), Node(id='Decoder Blocks', type='Component', properties={})], relationships=[Relationship(source=Node(id='Bert', type='Model', properties={}), target=Node(id='Decoder-Only Models', type='Models', properties={}), type='CONTRAST', properties={}), Relationship(source=Node(id='Gpt-1', type='Model', properties={}), target=Node(id='Decoder-Only Models', type='Models', properties={}), type='INSTANCE', properties={}), Relationship(source=Node(id='Gpt-2', type='Model', properties={}), target=Node(id='Decoder-Only Models', type='Models', properties={}), type='INSTANCE', properties={}), Relationship(source=Node(id='Gpt-2', type='Model', properties={}), target=Node(id='Openai', type='Organization', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Decoder Blocks', type='Component', properties={}), target=Node(id='Gpt-1', type='Model', properties={}), type='COMPONENT_OF', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.\\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use')), GraphDocument(nodes=[Node(id='Openai', type='Organization', properties={}), Node(id='Gpt-3', type='Model', properties={}), Node(id='Gpt-4', type='Model', properties={}), Node(id='Chatgpt', type='Chatbot', properties={})], relationships=[Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Gpt-3', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Gpt-4', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Chatgpt', type='Chatbot', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Gpt-3', type='Model', properties={}), target=Node(id='Gpt-4', type='Model', properties={}), type='PREDECESSOR', properties={}), Relationship(source=Node(id='Gpt-4', type='Model', properties={}), target=Node(id='Chatgpt', type='Chatbot', properties={}), type='BASIS', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' OpenAI claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2025 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing chatbot ChatGPT that received extensive media coverage and public attention. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities')), GraphDocument(nodes=[Node(id='Openai', type='Organization', properties={}), Node(id='Gpt-4', type='Model', properties={}), Node(id='Chatgpt', type='Model', properties={}), Node(id='Openai O1', type='Model', properties={}), Node(id='Robotics', type='Field', properties={}), Node(id='Software Engineering', type='Field', properties={}), Node(id='Societal Impact Work', type='Field', properties={})], relationships=[Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Gpt-4', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Chatgpt', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Openai O1', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Chatgpt', type='Model', properties={}), target=Node(id='Robotics', type='Field', properties={}), type='INFLUENCED', properties={}), Relationship(source=Node(id='Chatgpt', type='Model', properties={}), target=Node(id='Software Engineering', type='Field', properties={}), type='INFLUENCED', properties={}), Relationship(source=Node(id='Chatgpt', type='Model', properties={}), target=Node(id='Societal Impact Work', type='Field', properties={}), type='INFLUENCED', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal the high-level architecture and the number of parameters of GPT-4. The release of ChatGPT led to an uptick in LLM usage across several research subfields of computer science, including robotics, software engineering, and societal impact work. In 2024 OpenAI released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer')), GraphDocument(nodes=[Node(id='Openai', type='Organization', properties={}), Node(id='O1', type='Model', properties={}), Node(id='Gpt Series', type='Model', properties={}), Node(id='Bloom', type='Model', properties={}), Node(id='Llama', type='Model', properties={}), Node(id='Mistral Ai', type='Organization', properties={}), Node(id='Mistral 7B', type='Model', properties={}), Node(id='Mixtral 8X7B', type='Model', properties={})], relationships=[Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='O1', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Openai', type='Organization', properties={}), target=Node(id='Gpt Series', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Mistral Ai', type='Organization', properties={}), target=Node(id='Mistral 7B', type='Model', properties={}), type='DEVELOPER', properties={}), Relationship(source=Node(id='Mistral Ai', type='Organization', properties={}), target=Node(id='Mixtral 8X7B', type='Model', properties={}), type='DEVELOPER', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=\" released the reasoning model OpenAI o1, which generates long chains of thought before returning a final answer. Many LLMs with parameter counts comparable to those of OpenAI's GPT series have been developed.\\nSince 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License\")), GraphDocument(nodes=[Node(id='Mistral 7B', type='Model', properties={}), Node(id='Mixtral 8X7B', type='Model', properties={}), Node(id='Apache License', type='License', properties={}), Node(id='Deepseek', type='Organization', properties={}), Node(id='Deepseek R1', type='Model', properties={})], relationships=[Relationship(source=Node(id='Mistral 7B', type='Model', properties={}), target=Node(id='Apache License', type='License', properties={}), type='LICENSE', properties={}), Relationship(source=Node(id='Mixtral 8X7B', type='Model', properties={}), target=Node(id='Apache License', type='License', properties={}), type='LICENSE', properties={}), Relationship(source=Node(id='Deepseek', type='Organization', properties={}), target=Node(id='Deepseek R1', type='Model', properties={}), type='RELEASED', properties={})], source=Document(metadata={'title': 'Large language model', 'summary': 'A large language model (LLM) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation.\\nThe largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.', 'source': 'https://en.wikipedia.org/wiki/Large_language_model'}, page_content=' models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. In January 2025, DeepSeek released DeepSeek R1, a 671-billion-parameter open-weight model that performs c')), GraphDocument(nodes=[Node(id='Large Language Model', type='Machine learning model', properties={}), Node(id='Natural Language Processing', type='Task', properties={}), Node(id='Language Generation', type='Task', properties={}), Node(id='Self-Supervised Learning', type='Learning method', properties={})], relationships=[Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Natural Language Processing', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Language Generation', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Large Language Model', type='Machine learning model', properties={}), target=Node(id='Self-Supervised Learning', type='Learning method', properties={}), type='TRAINED_WITH', properties={})], source=Document(metadata={'title': 'List of large language models', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.', 'source': 'https://en.wikipedia.org/wiki/List_of_large_language_models'}, page_content='A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.\\n\\n\\n== List ==\\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec √ó 1 day = 8.64E19 FLOP. Also')), GraphDocument(nodes=[Node(id='Petaflop/Sec', type='Unit of measurement', properties={}), Node(id='Day', type='Unit of time', properties={}), Node(id='Flop', type='Unit of measurement', properties={})], relationships=[Relationship(source=Node(id='Petaflop/Sec', type='Unit of measurement', properties={}), target=Node(id='Flop', type='Unit of measurement', properties={}), type='CONVERSION', properties={})], source=Document(metadata={'title': 'List of large language models', 'summary': 'A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\\nThis page lists notable large language models.', 'source': 'https://en.wikipedia.org/wiki/List_of_large_language_models'}, page_content=\" 1 petaFLOP/sec √ó 1 day = 8.64E19 FLOP. Also, only the largest model's cost is written.\\n\\n\\n== See also ==\\nList of chatbots\\nList of language model benchmarks\\n\\n\\n== Notes ==\\n\\n\\n== References ==\")), GraphDocument(nodes=[Node(id='Language Model', type='Concept', properties={}), Node(id='Human Brain', type='Organ', properties={}), Node(id='Speech Recognition', type='Task', properties={}), Node(id='Machine Translation', type='Task', properties={}), Node(id='Natural Language Generation', type='Task', properties={}), Node(id='Optical Character Recognition', type='Task', properties={}), Node(id='Route Optimization', type='Task', properties={}), Node(id='Handwriting Recognition', type='Task', properties={}), Node(id='Grammar Induction', type='Task', properties={}), Node(id='Information Retrieval', type='Task', properties={}), Node(id='Large Language Models', type='Concept', properties={}), Node(id='Transformers', type='Model', properties={}), Node(id='Public Internet', type='Dataset', properties={})], relationships=[Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Human Brain', type='Organ', properties={}), type='MODELS', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Speech Recognition', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Machine Translation', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Natural Language Generation', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Optical Character Recognition', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Route Optimization', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Handwriting Recognition', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Grammar Induction', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Language Model', type='Concept', properties={}), target=Node(id='Information Retrieval', type='Task', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Large Language Models', type='Concept', properties={}), target=Node(id='Transformers', type='Model', properties={}), type='BASED_ON', properties={}), Relationship(source=Node(id='Transformers', type='Model', properties={}), target=Node(id='Public Internet', type='Dataset', properties={}), type='TRAINED_ON', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=\"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They\")), GraphDocument(nodes=[Node(id='Transformers', type='Model', properties={}), Node(id='Recurrent Neural Network', type='Model', properties={}), Node(id='Statistical Models', type='Model', properties={}), Node(id='Noam Chomsky', type='Person', properties={}), Node(id='Formal Grammars', type='Theory', properties={})], relationships=[Relationship(source=Node(id='Transformers', type='Model', properties={}), target=Node(id='Recurrent Neural Network', type='Model', properties={}), type='SUPERSEDED', properties={}), Relationship(source=Node(id='Recurrent Neural Network', type='Model', properties={}), target=Node(id='Statistical Models', type='Model', properties={}), type='SUPERSEDED', properties={}), Relationship(source=Node(id='Noam Chomsky', type='Person', properties={}), target=Node(id='Formal Grammars', type='Theory', properties={}), type='DEVELOPED', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\\n\\n\\n== History ==\\nNoam Chomsky did pioneering work on language models in the 1950s by developing a theory of formal grammars.\\nIn 1980, statistical approaches were explored and found to be more useful for many purposes than rule-based')), GraphDocument(nodes=[Node(id='Statistical Approaches', type='Approach', properties={}), Node(id='Rule-Based Formal Grammars', type='Grammar', properties={}), Node(id='Discrete Representations', type='Representation', properties={}), Node(id='Word N-Gram Language Models', type='Model', properties={}), Node(id='Continuous Representations', type='Representation', properties={}), Node(id='Word Embeddings', type='Representation', properties={})], relationships=[Relationship(source=Node(id='Statistical Approaches', type='Approach', properties={}), target=Node(id='Discrete Representations', type='Representation', properties={}), type='UTILIZES', properties={}), Relationship(source=Node(id='Discrete Representations', type='Representation', properties={}), target=Node(id='Word N-Gram Language Models', type='Model', properties={}), type='INCLUDES', properties={}), Relationship(source=Node(id='Continuous Representations', type='Representation', properties={}), target=Node(id='Word Embeddings', type='Representation', properties={}), type='INCLUDES', properties={}), Relationship(source=Node(id='Word Embeddings', type='Representation', properties={}), target=Node(id='Discrete Representations', type='Representation', properties={}), type='REPLACES', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='In 1980, statistical approaches were explored and found to be more useful for many purposes than rule-based formal grammars. Discrete representations like word n-gram language models, with probabilities for discrete combinations of words, made significant advances.\\nIn the 2000s, continuous representations for words, such as word embeddings, began to replace discrete representations. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer')), GraphDocument(nodes=[Node(id='Word', type='Word', properties={}), Node(id='Vector Space', type='Vector space', properties={}), Node(id='Statistical Language Model', type='Statistical language model', properties={}), Node(id='Ibm', type='Organization', properties={}), Node(id='Human Subjects', type='Human subjects', properties={})], relationships=[Relationship(source=Node(id='Word', type='Word', properties={}), target=Node(id='Vector Space', type='Vector space', properties={}), type='ENCODING', properties={}), Relationship(source=Node(id='Statistical Language Model', type='Statistical language model', properties={}), target=Node(id='Ibm', type='Organization', properties={}), type='DEVELOPED', properties={}), Relationship(source=Node(id='Ibm', type='Organization', properties={}), target=Node(id='Human Subjects', type='Human subjects', properties={}), type='PERFORMED_EXPERIMENTS', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning, and common relationships between pairs of words like plurality or gender.\\n\\n\\n== Pure statistical models ==\\nIn 1980, the first significant statistical language model was proposed, and during the decade IBM performed ‚ÄòShannon-style‚Äô experiments, in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in')), GraphDocument(nodes=[Node(id='Language Modeling', type='Concept', properties={}), Node(id='Word N-Grams', type='Model', properties={}), Node(id='Exponential', type='Model', properties={}), Node(id='Maximum Entropy Language Models', type='Model', properties={})], relationships=[Relationship(source=Node(id='Language Modeling', type='Concept', properties={}), target=Node(id='Word N-Grams', type='Model', properties={}), type='USES', properties={}), Relationship(source=Node(id='Word N-Grams', type='Model', properties={}), target=Node(id='Exponential', type='Model', properties={}), type='INCLUDES', properties={}), Relationship(source=Node(id='Exponential', type='Model', properties={}), target=Node(id='Maximum Entropy Language Models', type='Model', properties={}), type='RELATES_TO', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' in which potential sources for language modeling improvement were identified by observing and analyzing the performance of human subjects in predicting or correcting text.\\n\\n\\n=== Models based on word n-grams ===\\n\\n\\n=== Exponential ===\\nMaximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The equation is\\n\\n  \\n    \\n      \\n        P\\n        (')), GraphDocument(nodes=[Node(id='P', type='Statement', properties={}), Node(id='W', type='Condition', properties={}), Node(id='M', type='Condition', properties={})], relationships=[Relationship(source=Node(id='P', type='Statement', properties={}), target=Node(id='W', type='Condition', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='P', type='Statement', properties={}), target=Node(id='M', type='Condition', properties={}), type='DEPENDS_ON', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='  \\n        P\\n        (\\n        \\n          w\\n          \\n            m\\n          \\n        \\n        ‚à£\\n     ')), GraphDocument(nodes=[Node(id='W', type='Variable', properties={}), Node(id='1', type='Number', properties={})], relationships=[Relationship(source=Node(id='W', type='Variable', properties={}), target=Node(id='1', type='Number', properties={}), type='ASSOCIATED_WITH', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    \\n        ‚à£\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        ‚Ä¶\\n   ')), GraphDocument(nodes=[Node(id='W', type='Variable', properties={}), Node(id='M', type='Variable', properties={}), Node(id='1', type='Number', properties={})], relationships=[Relationship(source=Node(id='W', type='Variable', properties={}), target=Node(id='M', type='Variable', properties={}), type='RELATED_TO', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='       ,\\n        ‚Ä¶\\n        ,\\n        \\n          w\\n          \\n            m\\n            ‚àí\\n            1\\n    ')), GraphDocument(nodes=[Node(id='Equation', type='Mathematical expression', properties={}), Node(id='Numerator', type='Number', properties={}), Node(id='Denominator', type='Number', properties={})], relationships=[Relationship(source=Node(id='Equation', type='Mathematical expression', properties={}), target=Node(id='Numerator', type='Number', properties={}), type='HAS_NUMERATOR', properties={}), Relationship(source=Node(id='Equation', type='Mathematical expression', properties={}), target=Node(id='Denominator', type='Number', properties={}), type='HAS_DENOMINATOR', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='  ‚àí\\n            1\\n          \\n        \\n        )\\n        =\\n        \\n          \\n            1\\n            \\n')), GraphDocument(nodes=[Node(id='Z', type='Entity', properties={}), Node(id='W', type='Entity', properties={})], relationships=[Relationship(source=Node(id='Z', type='Entity', properties={}), target=Node(id='W', type='Entity', properties={}), type='RELATED', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      1\\n            \\n              Z\\n              (\\n              \\n                w\\n                \\n ')), GraphDocument(nodes=[Node(id='W', type='Variable', properties={}), Node(id='1', type='Number', properties={})], relationships=[Relationship(source=Node(id='W', type='Variable', properties={}), target=Node(id='1', type='Number', properties={}), type='EQUALS', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' w\\n                \\n                  1\\n                \\n              \\n              ,\\n              ‚Ä¶\\n')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    ,\\n              ‚Ä¶\\n              ,\\n              \\n                w\\n                \\n                ')), GraphDocument(nodes=[Node(id='M', type='Variable', properties={}), Node(id='1', type='Number', properties={})], relationships=[Relationship(source=Node(id='M', type='Variable', properties={}), target=Node(id='1', type='Number', properties={}), type='SUBTRACTION', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='   \\n                  m\\n                  ‚àí\\n                  1\\n                \\n              \\n       ')), GraphDocument(nodes=[Node(id='Exp', type='Mathematical function', properties={})], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='            \\n              )\\n            \\n          \\n        \\n        exp\\n        \\u2061\\n        (\\n        \\n ')), GraphDocument(nodes=[Node(id='A', type='Variable', properties={}), Node(id='T', type='Variable', properties={}), Node(id='F', type='Function', properties={})], relationships=[Relationship(source=Node(id='F', type='Function', properties={}), target=Node(id='A', type='Variable', properties={}), type='INPUT', properties={}), Relationship(source=Node(id='F', type='Function', properties={}), target=Node(id='T', type='Variable', properties={}), type='INPUT', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='\\n        (\\n        \\n          a\\n          \\n            T\\n          \\n        \\n        f\\n        (\\n        ')), GraphDocument(nodes=[Node(id='W', type='Unknown', properties={})], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='  f\\n        (\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        ‚Ä¶\\n      ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    ,\\n        ‚Ä¶\\n        ,\\n        \\n          w\\n          \\n            m\\n          \\n        \\n        )\\n    ')), GraphDocument(nodes=[Node(id='P', type='Probability', properties={}), Node(id='W_M', type='Word', properties={}), Node(id='W_1', type='Word', properties={}), Node(id='Z', type='Normalization constant', properties={}), Node(id='A', type='Vector', properties={}), Node(id='F', type='Function', properties={})], relationships=[Relationship(source=Node(id='P', type='Probability', properties={}), target=Node(id='W_M', type='Word', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='P', type='Probability', properties={}), target=Node(id='Z', type='Normalization constant', properties={}), type='NORMALIZED_BY', properties={}), Relationship(source=Node(id='Z', type='Normalization constant', properties={}), target=Node(id='W_1', type='Word', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='A', type='Vector', properties={}), target=Node(id='F', type='Function', properties={}), type='INPUT_TO', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle P(w_{m}\\\\mid w_{1},\\\\ldots ,w_{m-1})={\\\\frac {1}{Z(w_{1},\\\\ldots ,w_{m-1})}}\\\\exp(a^{T}f(w_{1},')), GraphDocument(nodes=[Node(id='Z', type='Function', properties={}), Node(id='W', type='Variable', properties={}), Node(id='A', type='Variable', properties={}), Node(id='F', type='Function', properties={})], relationships=[Relationship(source=Node(id='Z', type='Function', properties={}), target=Node(id='W', type='Variable', properties={}), type='INPUT', properties={}), Relationship(source=Node(id='F', type='Function', properties={}), target=Node(id='W', type='Variable', properties={}), type='INPUT', properties={}), Relationship(source=Node(id='A', type='Variable', properties={}), target=Node(id='F', type='Function', properties={}), type='INPUT', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='w_{m-1})}}\\\\exp(a^{T}f(w_{1},\\\\ldots ,w_{m}))}\\n  \\n\\nwhere \\n  \\n    \\n      \\n        Z\\n        (\\n        \\n          w\\n         ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='          w\\n          \\n            1\\n          \\n        \\n        ,\\n        ‚Ä¶\\n        ,\\n        \\n         ')), GraphDocument(nodes=[Node(id='W', type='Variable', properties={}), Node(id='M-1', type='Value', properties={})], relationships=[Relationship(source=Node(id='W', type='Variable', properties={}), target=Node(id='M-1', type='Value', properties={}), type='EQUALS', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' ,\\n        \\n          w\\n          \\n            m\\n            ‚àí\\n            1\\n          \\n        \\n        )')), GraphDocument(nodes=[Node(id='Z', type='Partition function', properties={}), Node(id='W1', type='Variable', properties={}), Node(id='Wm-1', type='Variable', properties={}), Node(id='A', type='Variable', properties={})], relationships=[Relationship(source=Node(id='Z', type='Partition function', properties={}), target=Node(id='W1', type='Variable', properties={}), type='DEPENDS_ON', properties={}), Relationship(source=Node(id='Z', type='Partition function', properties={}), target=Node(id='Wm-1', type='Variable', properties={}), type='DEPENDS_ON', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='  \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z(w_{1},\\\\ldots ,w_{m-1})}\\n  \\n is the partition function, \\n  \\n    \\n      \\n        a\\n      \\n  ')), GraphDocument(nodes=[Node(id='A', type='Parameter vector', properties={}), Node(id='F', type='Function', properties={}), Node(id='W', type='Input', properties={})], relationships=[Relationship(source=Node(id='F', type='Function', properties={}), target=Node(id='W', type='Input', properties={}), type='TAKES_INPUT', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content=' \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  \\n is the parameter vector, and \\n  \\n    \\n      \\n        f\\n        (\\n        \\n          w\\n    ')), GraphDocument(nodes=[], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='    \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        ‚Ä¶\\n        ,\\n        \\n    ')), GraphDocument(nodes=[Node(id='F', type='Function', properties={})], relationships=[], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      ,\\n        \\n          w\\n          \\n            m\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f(')), GraphDocument(nodes=[Node(id='Feature Function', type='Function', properties={}), Node(id='F(W_{1},\\\\Ldots ,W_{M})', type='Mathematical expression', properties={})], relationships=[Relationship(source=Node(id='Feature Function', type='Function', properties={}), target=Node(id='F(W_{1},\\\\Ldots ,W_{M})', type='Mathematical expression', properties={}), type='REPRESENTS', properties={})], source=Document(metadata={'title': 'Language model', 'summary': \"A language model is a model of the human brain's ability to produce natural language. Language models are useful for a variety of tasks, including speech recognition, machine translation, natural language generation (generating more human-like text), optical character recognition, route optimization, handwriting recognition, grammar induction, and information retrieval.\\nLarge language models (LLMs), currently their most advanced form, are predominantly based on transformers trained on larger datasets (frequently using texts scraped from the public internet). They have superseded recurrent neural network-based models, which had previously superseded the purely statistical models, such as the word n-gram language model.\", 'source': 'https://en.wikipedia.org/wiki/Language_model'}, page_content='      \\n    \\n    {\\\\displaystyle f(w_{1},\\\\ldots ,w_{m})}\\n  \\n is the feature function. In the simplest case, the feature function is just an ind'))]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import ChatGroq (LLM wrapper) and LLMGraphTransformer for graph generation\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer \n",
    "\n",
    "# Step 2: Load environment variables (e.g., API keys) from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Step 3: Initialize the LLM (Groq's LLaMA 3.3 model)\n",
    "# temperature controls randomness; max_tokens limits the output length\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "# Step 4: Wrap the LLM with a graph transformer to extract entities and relationships\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)  \n",
    "\n",
    "# Step 5: Convert document chunks into structured graph documents (nodes + edges)\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents) \n",
    "\n",
    "# Step 6: Print the resulting graph structure for inspection\n",
    "print(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5ec7e",
   "metadata": {},
   "source": [
    "```\n",
    "[GraphDocument( \n",
    "    nodes=[ \n",
    "        Node(id='Llm', type='Computational model'), \n",
    "        Node(id='Language Generation', type='Concept'), \n",
    "        Node(id='Natural Language Processing Tasks', type='Concept'), \n",
    "        Node(id='Llama Family', type='Computational model'), \n",
    "        Node(id='Ibm', type='Organization'), \n",
    "        ..., Node(id='Bert', type='Computational model')], \n",
    "    relationships=[ \n",
    "        Relationship(source=Node(id='Llm', type='Computational model'), \n",
    "                     target=Node(id='Language Generation', type='Concept'), \n",
    "                     type='CAPABLE_OF'), \n",
    "        ...])] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec52ad9",
   "metadata": {},
   "source": [
    "![img_3](https://github.com/mohd-faizy/Developing_LLMs_Applications_with_LangChain/blob/main/_img/0603.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d729a3",
   "metadata": {},
   "source": [
    "## **‚≠ê02: Storing and Querying documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd55c560",
   "metadata": {},
   "source": [
    "![img_4](https://github.com/mohd-faizy/Developing_LLMs_Applications_with_LangChain/blob/main/_img/0604.jpeg?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a6f41",
   "metadata": {},
   "source": [
    "Once graph documents are prepared, we store them in a `Neo4j` database and interact using Cypher queries.\n",
    "\n",
    "\n",
    "#### üß† What is **Neo4j**?\n",
    "\n",
    "* **Neo4j** is a **graph database** that stores data in **nodes** (entities) and **relationships** (connections between entities).\n",
    "* Unlike traditional relational databases, it excels at handling **complex, interconnected data** ‚Äî like `social networks`, `knowledge graphs`, `recommendation systems`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîó What is its role in **LangChain**/**LangGraph**?\n",
    "\n",
    "* In **LangChain**/**LangGraph**, Neo4j is used to:\n",
    "\n",
    "  * Store and retrieve structured knowledge (as a **Knowledge Graph**).\n",
    "  * Help language models **reason** better by providing **contextual, structured data**.\n",
    "  * Support **RAG** (Retrieval-Augmented Generation) pipelines where the LLM queries the knowledge graph for better answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why use Neo4j with LangChain / LangGraph?\n",
    "\n",
    "* **Structured memory**: It acts like a memory graph for agents ‚Äî \"who met whom, when, where, and why\".\n",
    "* **Better reasoning**: LLMs can retrieve and reason over structured data instead of guessing from text alone.\n",
    "* **Entity linking**: Tracks and connects information about entities like people, places, or products.\n",
    "* **Dynamic updates**: You can update the graph over time with new info as the agent learns.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîç Example Use Case\n",
    "\n",
    "> Imagine you're building an **AI agent that acts like a personal assistant**.\n",
    "\n",
    "* The assistant talks to you daily and stores facts:\n",
    "\n",
    "  * \"Alex likes leg day.\"\n",
    "  * \"Alex has a herniated disc at L4-L5.\"\n",
    "  * \"Avoids barbell squats.\"\n",
    "* These facts are stored in **Neo4j as a knowledge graph**:\n",
    "\n",
    "  ```plaintext\n",
    "  (:Person {name: \"alex\"})-[:LIKES]->(:Activity {name: \"leg day\"})\n",
    "  (:Person {name: \"alex\"})-[:HAS_INJURY]->(:Injury {type: \"herniated disc\", location: \"L4-L5\"})\n",
    "  ```\n",
    "\n",
    "When the assistant is asked:\n",
    "\n",
    "> \"Should I do squats today?\"\n",
    "\n",
    "It can reason: \"Alex has a herniated disc ‚Üí avoid heavy spinal loading ‚Üí no barbell squats.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è How to Use Neo4j with LangChain (Code Example)\n",
    "\n",
    "> üõ† **Install dependencies:**\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-neo4j neo4j\n",
    "```\n",
    "\n",
    "> üß± **Set up Neo4j graph in LangChain:**\n",
    "\n",
    "```python\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"your_password\"\n",
    ")\n",
    "```\n",
    "\n",
    "> üì• **Ingest data into the graph:**\n",
    "\n",
    "```python\n",
    "graph.query(\"\"\"\n",
    "MERGE (p:Person {name: 'Faizy'})\n",
    "MERGE (a:Activity {name: 'leg day'})\n",
    "MERGE (p)-[:LIKES]->(a)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "> üîç **Query the graph using Cypher:**\n",
    "\n",
    "```python\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (p:Person)-[:LIKES]->(a:Activity)\n",
    "WHERE p.name = 'Faizy'\n",
    "RETURN a.name\n",
    "\"\"\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Summary\n",
    "\n",
    "| Feature | Neo4j in LangChain                                      |\n",
    "| ------- | ------------------------------------------------------- |\n",
    "| Type    | Graph Database                                          |\n",
    "| Purpose | Store structured knowledge (nodes/edges)                |\n",
    "| Benefit | Enables reasoning and memory for agents                 |\n",
    "| Example | Personal assistant storing health + workout preferences |\n",
    "| Tools   | `langchain-neo4j`, Cypher queries                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2cc494",
   "metadata": {},
   "source": [
    "![img_5](https://github.com/mohd-faizy/Developing_LLMs_Applications_with_LangChain/blob/main/_img/0605.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef68252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import Required Libraries ===\n",
    "from langchain_neo4j import Neo4jGraph  # Interface to connect and interact with Neo4j\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_groq import ChatGroq  # LLM provider (Groq's LLaMA models)\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv  # Load environment variables from .env\n",
    "import os\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# === Connect to Neo4j ===\n",
    "# Establishes a session with the Neo4j graph database\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "# === Initialize the LLM and Graph Transformer ===\n",
    "# ChatGroq provides LLaMA-3.1 model; used to extract graph-structured info\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# === Convert document chunks into graph format (nodes + relationships) ===\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "# === Store graph documents in Neo4j ===\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    include_source=True,   # Adds MENTIONS edges linking nodes to source docs\n",
    "    baseEntityLabel=True   # Adds a generic __Entity__ label to all nodes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e86dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the graph schema to sync with the latest database structure\n",
    "graph.refresh_schema()\n",
    "\n",
    "# Print the current schema: shows node labels, properties, and relationship types\n",
    "print(graph.get_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this mini-test:\n",
    "print(graph.query(\"MATCH (n) RETURN COUNT(n)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804835b",
   "metadata": {},
   "source": [
    "```\n",
    "Node properties: \n",
    "Concept {id: STRING} \n",
    "Architecture {id: STRING} \n",
    "Organization {id: STRING} \n",
    "Event {id: STRING} \n",
    "Paper {id: STRING} \n",
    "The relationships: \n",
    "(:Concept)-[:DEVELOPED_BY]->(:Person) \n",
    "(:Architecture)-[:BASED_ON]->(:Concept) \n",
    "(:Organization)-[:PROPOSED]->(:Concept) \n",
    "(:Document)-[:MENTIONS]->(:Event) \n",
    "(:Paper)-[:BASED_ON]->(:Concept) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a218f",
   "metadata": {},
   "source": [
    "```python\n",
    "# verify the Bolt connection separately\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"your_password\"))\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"RETURN 1\")\n",
    "    print(result.single())\n",
    "```\n",
    "If you get a number, connection works. If you still get ConnectionRefusedError, then the database isn't running or port isn't open."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d188dd",
   "metadata": {},
   "source": [
    "## **‚≠ê03: Creating the RAG Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eca2e9",
   "metadata": {},
   "source": [
    "![img_6](https://github.com/mohd-faizy/Developing_LLMs_Applications_with_LangChain/blob/main/_img/0606.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76449829",
   "metadata": {},
   "source": [
    "We now build the retrieval-augmented generation chain using graph context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b863a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Cypher-based QA chain for graph querying\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain  \n",
    "\n",
    "# Initialize the LLM (Groq's LLaMA 3.1) for Cypher generation and answer formatting\n",
    "# llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7, max_tokens=100)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Create a QA chain that uses the LLM and connected Neo4j graph\n",
    "chain = GraphCypherQAChain.from_llm( \n",
    "    llm=llm,\n",
    "    graph=graph, \n",
    "    verbose=True  # Prints generated Cypher query and execution details\n",
    ")  \n",
    "\n",
    "# Ask a natural language question ‚Üí LLM generates a Cypher query ‚Üí executes on Neo4j\n",
    "result = chain.invoke({\"query\": \"What is the most accurate model?\"}) \n",
    "\n",
    "# Print the final answer from the graph database\n",
    "print(f\"Final answer: {result['result']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2a483",
   "metadata": {},
   "source": [
    "```\n",
    "> Entering new GraphCypherQAChain chain... \n",
    "  Generated Cypher: \n",
    "  MATCH (m:Model) \n",
    "  RETURN m \n",
    "  ORDER BY m.accuracy DESC \n",
    "  LIMIT 1; \n",
    "  Full Context: \n",
    "  [{'m': {'id': 'Artificial Neural Networks'}}] \n",
    "\n",
    "> Finished chain. \n",
    "\n",
    "> Final answer: Artificial Neural Networks \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b9bd6",
   "metadata": {},
   "source": [
    "- `qa_prompt` : Prompt template for result generation\n",
    "- `cypher_prompt` : Prompt template for Cypher generation\n",
    "- `cypher_llm` : LLM for Cypher generation\n",
    "- `qa_llm` : LLM for result generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1363103",
   "metadata": {},
   "source": [
    "## **‚≠ê04: Improving Graph Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c86008",
   "metadata": {},
   "source": [
    "#### **Techniques**\n",
    "\n",
    ">`Main limitation: reliability of user ‚Üí Cypher translation`\n",
    "\n",
    "##### **Strategies to improve graph retrieval system:**\n",
    "- ***1. Filtering Graph Schema***\n",
    "- ***2. Validating the Cypher Query***\n",
    "- ***3. Few-shot prompting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ce71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================\n",
    "# 1. Filtering \n",
    "#=====================\n",
    "\n",
    "# Import the GraphCypherQAChain for querying Neo4j using natural language\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain  \n",
    "\n",
    "# Initialize the LLM (LLaMA 3.1 from Groq)\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7, max_tokens=100)\n",
    "\n",
    "# Create a QA chain with filtering enabled\n",
    "# exclude_types=[\"Concept\"] will ignore 'Concept' nodes in generated Cypher queries\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    exclude_types=[\"Concept\"], \n",
    "    verbose=True  # Show generated Cypher and execution logs\n",
    ")\n",
    "\n",
    "# Print the current graph schema (useful to verify node types being filtered)\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049aefb",
   "metadata": {},
   "source": [
    "```\n",
    "Node properties: \n",
    "Document {title: STRING, id: STRING, text: STRING, summary: STRING, source: STRING} \n",
    "Organization {id: STRING} \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# 2. Validating the Cypher query\n",
    "#================================\n",
    "\n",
    "# Create a QA chain with Cypher validation enabled\n",
    "# validate_cypher=True ensures the generated query:\n",
    "# - follows schema rules\n",
    "# - uses correct relationship directions\n",
    "# - avoids invalid node/edge types\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    validate_cypher=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90089ee7",
   "metadata": {},
   "source": [
    "1. Detects nodes and relationships\n",
    "2. Determines the directions of the relationship\n",
    "3. Checks the graph schema\n",
    "4. Update the direction of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d22091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# 3. Few-shot prompting\n",
    "#======================\n",
    "\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate  \n",
    "\n",
    "# Define a few-shot set: sample questions and their correct Cypher queries\n",
    "examples = [ \n",
    "    { \n",
    "        \"question\": \"How many notable large language models are mentioned in the article?\", \n",
    "        \"query\": \"MATCH (m:Concept {id: 'Large Language Model'}) RETURN count(DISTINCT m)\", \n",
    "    }, \n",
    "    { \n",
    "        \"question\": \"Which companies or organizations have developed the large language models mentioned?\", \n",
    "        \"query\": \"MATCH (o:Organization)-[:DEVELOPS]->(m:Concept {id: 'Large Language Model'}) RETURN DISTINCT o.id\", \n",
    "    }, \n",
    "    { \n",
    "        \"question\": \"What is the largest model size mentioned in the article, in terms of number of parameters?\", \n",
    "        \"query\": \"MATCH (m:Concept {id: 'Large Language Model'}) RETURN max(m.parameters) AS largest_model\", \n",
    "    }, \n",
    "]\n",
    "\n",
    "# Format each example into a consistent prompt structure\n",
    "example_prompt = PromptTemplate.from_template(\"User input: {question}\\nCypher query: {query}\")  \n",
    "\n",
    "# Construct the few-shot Cypher prompt with schema injection\n",
    "cypher_prompt = FewShotPromptTemplate( \n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    prefix=\"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct\n",
    "            Cypher query to run.\\n\\nHere is the schema information\\n{schema}.\\n\\n\n",
    "            Below are a number of examples of questions and their corresponding Cypher queries.\"\"\", \n",
    "    suffix=\"User input: {question}\\nCypher query: \", \n",
    "    input_variables=[\"question\"], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================\n",
    "# Adding Few-shot Prompt to Chain\n",
    "#=================================\n",
    "\n",
    "# Create a QA chain with:\n",
    "# - Few-shot Cypher prompt for better query generation\n",
    "# - Cypher validation to ensure correctness\n",
    "# - Verbose mode to log internal steps\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    verbose=True,\n",
    "    validate_cypher=True \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Developing_LLMs_Applications_with_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
