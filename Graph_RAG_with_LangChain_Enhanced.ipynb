{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e479b61",
   "metadata": {},
   "source": [
    "# **ðŸ”·ðŸ”·Introduction to Graph RAGðŸ”·ðŸ”·**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a46733",
   "metadata": {},
   "source": [
    "## **â­01: From vectors to graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5f203",
   "metadata": {},
   "source": [
    "### **â­•Vector RAG Limitations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d333e",
   "metadata": {},
   "source": [
    "Vector-based RAG (Retrieval-Augmented Generation) systems combine **vector similarity search** with **generative models** (like GPT) to improve response accuracy by fetching relevant information from a knowledge base. While powerful, **Vector RAG has several limitations**, especially in real-world applications.\n",
    "\n",
    "Hereâ€™s a breakdown of the key limitations:\n",
    "\n",
    "\n",
    "\n",
    "#### **1. Semantic Drift or Irrelevant Retrieval**\n",
    "\n",
    "**Problem:** Vector search retrieves documents based on *semantic similarity*, not factual accuracy or context relevance.\n",
    "\n",
    "**Example:**\n",
    "Suppose a user asks:\n",
    "\n",
    "> \"What are the side effects of metformin?\"\n",
    "\n",
    "The system may retrieve chunks containing:\n",
    "\n",
    "* \"The benefits of metformin in managing blood sugar\"\n",
    "* \"Metformin's role in preventing diabetes complications\"\n",
    "\n",
    "**Issue:** These are related **semantically**, but they may **not directly mention side effects**. The model may then hallucinate or guess side effects, leading to unreliable answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Loss of Granularity**\n",
    "\n",
    "**Problem:** When documents are chunked too large or too small, RAG systems struggle.\n",
    "\n",
    "**Example:**\n",
    "A 20-page medical paper is chunked into 1-page chunks. The user's query:\n",
    "\n",
    "> \"What did the clinical trial say about metformin and weight loss?\"\n",
    "\n",
    "If the specific trial is mentioned briefly in just one paragraph, vector search may **miss it** if:\n",
    "\n",
    "* Chunk is too broad and dilutes relevance\n",
    "* Embedding doesn't capture that fine-grained detail\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **3. No True Understanding of Context**\n",
    "\n",
    "**Problem:** Vector similarity is context-agnostic â€” it doesnâ€™t consider previous turns in a conversation unless explicitly added.\n",
    "\n",
    "**Example:**\n",
    "**Conversation:**\n",
    "\n",
    "* User: \"Tell me about Python decorators.\"\n",
    "* Then: \"What about their use in Flask?\"\n",
    "\n",
    "Vector search may **not know what \"their\" refers to**, unless conversation history is added to the query, which RAG doesnâ€™t do natively.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **4. Stale or Outdated Knowledge**\n",
    "\n",
    "**Problem:** Vector RAG systems are only as up-to-date as their vector store.\n",
    "\n",
    "**Example:**\n",
    "User asks:\n",
    "\n",
    "> \"What's the latest security patch in Log4j?\"\n",
    "\n",
    "If the vector DB was last updated in 2022, it might **miss 2024 disclosures**, leading to outdated or dangerous information.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **5. Embedding Quality and Limitations**\n",
    "\n",
    "**Problem:** Embeddings might not capture important domain-specific nuances.\n",
    "\n",
    "**Example:**\n",
    "For a legal query like:\n",
    "\n",
    "> \"What precedent did the judge rely on in Roe v. Wade?\"\n",
    "\n",
    "Legal documents may contain dense citations and context-specific phrases. General-purpose embeddings may not **capture the key precedent accurately**, resulting in **weak or unrelated chunks** being retrieved.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **6. Long Documents, Poor Chunking**\n",
    "\n",
    "**Problem:** Poor chunking strategies lead to missed context or cutoff info.\n",
    "\n",
    "**Example:**\n",
    "In a user manual, the definition of \"safe operating temperature\" might be split across two chunks. The RAG system may retrieve only half of the definition, leading the model to guess or produce partial info.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **7. Lack of Ranking or Reranking**\n",
    "\n",
    "**Problem:** Vector RAG might retrieve the *top k* most similar documents but **not the most useful or trustworthy** ones.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "> \"What causes high CPU usage in Kubernetes?\"\n",
    "\n",
    "The top vector match might describe CPU metrics collection, while a lower-ranked chunk (ignored) contains the **actual explanation** for common CPU issues (e.g., liveness probes, throttling, etc.).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Limitation                       | Description                                         | Example                               |\n",
    "| -------------------------------- | --------------------------------------------------- | ------------------------------------- |\n",
    "| `Semantic Drift`                 | Retrieval is related but not precise                | Side effects vs benefits of metformin |\n",
    "| `Granularity Issues`             | Too broad/narrow chunks miss details                | Specific paragraph in long PDF missed |\n",
    "| `Lack of Conversational Context` | Fails to understand pronouns or dialogue continuity | \"What about their use?\" ambiguity     |\n",
    "| `Outdated Knowledge`             | Vector store isn't real-time                        | Security patch from 2024 missing      |\n",
    "| `Embedding Limitations`          | General embeddings miss domain-specific meaning     | Legal or medical terms                |\n",
    "| `Poor Chunking`                  | Important info split across boundaries              | Definition split between chunks       |\n",
    "| `No Reranking or Trust Score`    | Most relevant docs may be ranked lower              | Actual CPU issue not retrieved        |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51178dc6",
   "metadata": {},
   "source": [
    "### **â­•Graph Databases â€“ Nodes and Edges**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e6e44",
   "metadata": {},
   "source": [
    "Graph Databases â€“ Nodes and Edges in the context of Graph RAG (Retrieval-Augmented Generation) \n",
    "\n",
    "#### ðŸ” **Graph Databases â€“ Simple Explanation**\n",
    "\n",
    "* **Graph databases** store data like a web of connected dots (like a mind map).\n",
    "* These dots are called **Nodes** (represent real things like a person, model, or idea).\n",
    "* The lines between them are called **Edges** (show how those things are related).\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§© **Components**\n",
    "\n",
    "* **Nodes = Things (entities)**\n",
    "  Example:\n",
    "\n",
    "  * `Gpt-4` (a model)\n",
    "  * `OpenAI` (a company)\n",
    "\n",
    "* **Edges = Relationships between things**\n",
    "  Example:\n",
    "\n",
    "  * `Gpt-4` âž `DEVELOPED_BY` âž `OpenAI`\n",
    "  * `ChatGPT` âž `MENTIONS` âž `Gpt-4`\n",
    "  * `AutoGPT` âž `BASED_ON` âž `Gpt-4`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ¤– **How it's used in Graph RAG**\n",
    "\n",
    "* You can ask a question like:\n",
    "  *\"Who developed Gpt-4?\"*\n",
    "\n",
    "* The system follows the edge:\n",
    "\n",
    "  * Start at node `Gpt-4`\n",
    "  * Follow `DEVELOPED_BY` âž reach node `OpenAI`\n",
    "  * âœ… Answer: `OpenAI`\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… **Why Graphs are Useful**\n",
    "\n",
    "* Makes it easier to **trace relationships** step-by-step.\n",
    "* Helps LLMs **reason clearly** and **give explainable answers**.\n",
    "* Good for **complex questions** involving multiple steps or layers of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdf515",
   "metadata": {},
   "source": [
    "### **â­•Loading and Chunking Wikipedia Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc71a27",
   "metadata": {},
   "source": [
    "#### ðŸ”„ **How Unstructured Text is Converted into Graph Data**\n",
    "\n",
    "* ðŸ“š **Start with Wikipedia** as the knowledge source (it's full of useful info).\n",
    "* ðŸ” Use **WikipediaLoader** to get articles related to your question.\n",
    "* âœ‚ï¸ Split the article into smaller parts (chunks) using **TokenTextSplitter**\n",
    "*  âž¤ This helps the model handle the text properly (due to input size limits).\n",
    "* ðŸ§  These smaller chunks are then used to **build a graph**, making search and reasoning easier.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“ **Additional Points**\n",
    "\n",
    "* âœ… **Good chunking is important** â€“ It keeps the meaning of the text clear when reasoning later.\n",
    "* ðŸ”§ Graphs are created from these chunks using either:\n",
    "\n",
    "  * **LLMs (smart models)** or\n",
    "  * **Rule-based systems** (predefined logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f595e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import WikipediaLoader from langchain_community\n",
    "# This class allows us to programmatically fetch Wikipedia articles based on a query.\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# Step 2: Import TokenTextSplitter from langchain_text_splitters\n",
    "# TokenTextSplitter breaks down large documents into smaller, manageable chunks based on token counts.\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "\n",
    "# Step 3: Load Wikipedia documents related to a specific topic (e.g., \"large language model\")\n",
    "# This returns a list of Document objects, each containing content and metadata like title and source.\n",
    "raw_documents = WikipediaLoader(query=\"large language model\").load()\n",
    "\n",
    "# Step 4: Initialize a TokenTextSplitter\n",
    "# chunk_size = 100 â†’ each text chunk will be 100 tokens long\n",
    "# chunk_overlap = 20 â†’ 20 tokens from the previous chunk will be included in the next to maintain context\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# Step 5: Split the raw documents into smaller chunks\n",
    "# Splitting only the first 3 documents from the Wikipedia query result for simplicity/performance\n",
    "# This helps make downstream processing (e.g., embeddings, retrieval) more efficient and accurate\n",
    "documents = text_splitter.split_documents(raw_documents[:3])\n",
    "\n",
    "# Step 6: Print the first chunk to inspect its structure\n",
    "# Useful to verify what kind of data (page content + metadata) we are working with\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d444258",
   "metadata": {},
   "source": [
    "```\n",
    "page_content='A large language model (LLM) is a computational model capable of...'  \n",
    "metadata={'title': 'Large language model', \n",
    "          'summary': \"A large language model (LLM) is...\", \n",
    "          'source': 'https://en.wikipedia.org/wiki/Large_language_model'} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d90d44",
   "metadata": {},
   "source": [
    "### **â­•Converting Text to Graph Structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbaebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import ChatGroq (LLM wrapper) and LLMGraphTransformer for graph generation\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer \n",
    "\n",
    "# Step 2: Load environment variables (e.g., API keys) from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Step 3: Initialize the LLM (Groq's LLaMA 3.1 model)\n",
    "# temperature controls randomness; max_tokens limits the output length\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7, max_tokens=500)\n",
    "\n",
    "# Step 4: Wrap the LLM with a graph transformer to extract entities and relationships\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)  \n",
    "\n",
    "# Step 5: Convert document chunks into structured graph documents (nodes + edges)\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents) \n",
    "\n",
    "# Step 6: Print the resulting graph structure for inspection\n",
    "print(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5ec7e",
   "metadata": {},
   "source": [
    "```\n",
    "[GraphDocument( \n",
    "    nodes=[ \n",
    "        Node(id='Llm', type='Computational model'), \n",
    "        Node(id='Language Generation', type='Concept'), \n",
    "        Node(id='Natural Language Processing Tasks', type='Concept'), \n",
    "        Node(id='Llama Family', type='Computational model'), \n",
    "        Node(id='Ibm', type='Organization'), \n",
    "        ..., Node(id='Bert', type='Computational model')], \n",
    "    relationships=[ \n",
    "        Relationship(source=Node(id='Llm', type='Computational model'), \n",
    "                     target=Node(id='Language Generation', type='Concept'), \n",
    "                     type='CAPABLE_OF'), \n",
    "        ...])] \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d729a3",
   "metadata": {},
   "source": [
    "## **â­02: Storing and Querying documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a6f41",
   "metadata": {},
   "source": [
    "Once graph documents are prepared, we store them in a `Neo4j` database and interact using Cypher queries.\n",
    "\n",
    "\n",
    "#### ðŸ§  What is **Neo4j**?\n",
    "\n",
    "* **Neo4j** is a **graph database** that stores data in **nodes** (entities) and **relationships** (connections between entities).\n",
    "* Unlike traditional relational databases, it excels at handling **complex, interconnected data** â€” like `social networks`, `knowledge graphs`, `recommendation systems`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”— What is its role in **LangChain**/**LangGraph**?\n",
    "\n",
    "* In **LangChain**/**LangGraph**, Neo4j is used to:\n",
    "\n",
    "  * Store and retrieve structured knowledge (as a **Knowledge Graph**).\n",
    "  * Help language models **reason** better by providing **contextual, structured data**.\n",
    "  * Support **RAG** (Retrieval-Augmented Generation) pipelines where the LLM queries the knowledge graph for better answers.\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Why use Neo4j with LangChain / LangGraph?\n",
    "\n",
    "* **Structured memory**: It acts like a memory graph for agents â€” \"who met whom, when, where, and why\".\n",
    "* **Better reasoning**: LLMs can retrieve and reason over structured data instead of guessing from text alone.\n",
    "* **Entity linking**: Tracks and connects information about entities like people, places, or products.\n",
    "* **Dynamic updates**: You can update the graph over time with new info as the agent learns.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” Example Use Case\n",
    "\n",
    "> Imagine you're building an **AI agent that acts like a personal assistant**.\n",
    "\n",
    "* The assistant talks to you daily and stores facts:\n",
    "\n",
    "  * \"Alex likes leg day.\"\n",
    "  * \"Alex has a herniated disc at L4-L5.\"\n",
    "  * \"Avoids barbell squats.\"\n",
    "* These facts are stored in **Neo4j as a knowledge graph**:\n",
    "\n",
    "  ```plaintext\n",
    "  (:Person {name: \"alex\"})-[:LIKES]->(:Activity {name: \"leg day\"})\n",
    "  (:Person {name: \"alex\"})-[:HAS_INJURY]->(:Injury {type: \"herniated disc\", location: \"L4-L5\"})\n",
    "  ```\n",
    "\n",
    "When the assistant is asked:\n",
    "\n",
    "> \"Should I do squats today?\"\n",
    "\n",
    "It can reason: \"Alex has a herniated disc â†’ avoid heavy spinal loading â†’ no barbell squats.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### âš™ï¸ How to Use Neo4j with LangChain (Code Example)\n",
    "\n",
    "> ðŸ›  **Install dependencies:**\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-neo4j neo4j\n",
    "```\n",
    "\n",
    "> ðŸ§± **Set up Neo4j graph in LangChain:**\n",
    "\n",
    "```python\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"your_password\"\n",
    ")\n",
    "```\n",
    "\n",
    "> ðŸ“¥ **Ingest data into the graph:**\n",
    "\n",
    "```python\n",
    "graph.query(\"\"\"\n",
    "MERGE (p:Person {name: 'Faizy'})\n",
    "MERGE (a:Activity {name: 'leg day'})\n",
    "MERGE (p)-[:LIKES]->(a)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "> ðŸ” **Query the graph using Cypher:**\n",
    "\n",
    "```python\n",
    "result = graph.query(\"\"\"\n",
    "MATCH (p:Person)-[:LIKES]->(a:Activity)\n",
    "WHERE p.name = 'Faizy'\n",
    "RETURN a.name\n",
    "\"\"\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§© Summary\n",
    "\n",
    "| Feature | Neo4j in LangChain                                      |\n",
    "| ------- | ------------------------------------------------------- |\n",
    "| Type    | Graph Database                                          |\n",
    "| Purpose | Store structured knowledge (nodes/edges)                |\n",
    "| Benefit | Enables reasoning and memory for agents                 |\n",
    "| Example | Personal assistant storing health + workout preferences |\n",
    "| Tools   | `langchain-neo4j`, Cypher queries                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef68252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import Required Libraries ===\n",
    "from langchain_neo4j import Neo4jGraph  # Interface to connect and interact with Neo4j\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_groq import ChatGroq  # LLM provider (Groq's LLaMA models)\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv  # Load environment variables from .env\n",
    "import os\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "\n",
    "# === Get Neo4j connection credentials from environment ===\n",
    "url = os.getenv(\"NEO4J_URI\")           # e.g., bolt://localhost:7687\n",
    "user = os.getenv(\"NEO4J_USERNAME\")     # Username for Neo4j\n",
    "password = os.getenv(\"NEO4J_PASSWORD\") # Password for Neo4j\n",
    "\n",
    "# === Connect to Neo4j ===\n",
    "# Establishes a session with the Neo4j graph database\n",
    "graph = Neo4jGraph(url=url, username=user, password=password)\n",
    "\n",
    "# === Initialize the LLM and Graph Transformer ===\n",
    "# ChatGroq provides LLaMA-3.1 model; used to extract graph-structured info\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7, max_tokens=500)\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# === Convert document chunks into graph format (nodes + relationships) ===\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "# === Store graph documents in Neo4j ===\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    include_source=True,   # Adds MENTIONS edges linking nodes to source docs\n",
    "    baseEntityLabel=True   # Adds a generic __Entity__ label to all nodes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e86dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the graph schema to sync with the latest database structure\n",
    "graph.refresh_schema()\n",
    "\n",
    "# Print the current schema: shows node labels, properties, and relationship types\n",
    "print(graph.get_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this mini-test:\n",
    "print(graph.query(\"MATCH (n) RETURN COUNT(n)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804835b",
   "metadata": {},
   "source": [
    "```\n",
    "Node properties: \n",
    "Concept {id: STRING} \n",
    "Architecture {id: STRING} \n",
    "Organization {id: STRING} \n",
    "Event {id: STRING} \n",
    "Paper {id: STRING} \n",
    "The relationships: \n",
    "(:Concept)-[:DEVELOPED_BY]->(:Person) \n",
    "(:Architecture)-[:BASED_ON]->(:Concept) \n",
    "(:Organization)-[:PROPOSED]->(:Concept) \n",
    "(:Document)-[:MENTIONS]->(:Event) \n",
    "(:Paper)-[:BASED_ON]->(:Concept) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a218f",
   "metadata": {},
   "source": [
    "```python\n",
    "# verify the Bolt connection separately\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"your_password\"))\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"RETURN 1\")\n",
    "    print(result.single())\n",
    "```\n",
    "If you get a number, connection works. If you still get ConnectionRefusedError, then the database isn't running or port isn't open."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d188dd",
   "metadata": {},
   "source": [
    "## **â­03: Creating the RAG Chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76449829",
   "metadata": {},
   "source": [
    "We now build the retrieval-augmented generation chain using graph context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b863a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Cypher-based QA chain for graph querying\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain  \n",
    "\n",
    "# Initialize the LLM (Groq's LLaMA 3.1) for Cypher generation and answer formatting\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7, max_tokens=100)\n",
    "\n",
    "# Create a QA chain that uses the LLM and connected Neo4j graph\n",
    "chain = GraphCypherQAChain.from_llm( \n",
    "    llm=llm,\n",
    "    graph=graph, \n",
    "    verbose=True  # Prints generated Cypher query and execution details\n",
    ")  \n",
    "\n",
    "# Ask a natural language question â†’ LLM generates a Cypher query â†’ executes on Neo4j\n",
    "result = chain.invoke({\"query\": \"What is the most accurate model?\"}) \n",
    "\n",
    "# Print the final answer from the graph database\n",
    "print(f\"Final answer: {result['result']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a2a483",
   "metadata": {},
   "source": [
    "```\n",
    "> Entering new GraphCypherQAChain chain... \n",
    "  Generated Cypher: \n",
    "  MATCH (m:Model) \n",
    "  RETURN m \n",
    "  ORDER BY m.accuracy DESC \n",
    "  LIMIT 1; \n",
    "  Full Context: \n",
    "  [{'m': {'id': 'Artificial Neural Networks'}}] \n",
    "\n",
    "> Finished chain. \n",
    "\n",
    "> Final answer: Artificial Neural Networks \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b9bd6",
   "metadata": {},
   "source": [
    "- `qa_prompt` : Prompt template for result generation\n",
    "- `cypher_prompt` : Prompt template for Cypher generation\n",
    "- `cypher_llm` : LLM for Cypher generation\n",
    "- `qa_llm` : LLM for result generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1363103",
   "metadata": {},
   "source": [
    "## **â­04: Improving Graph Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c86008",
   "metadata": {},
   "source": [
    "#### **Techniques**\n",
    "\n",
    ">`Main limitation: reliability of user â†’ Cypher translation`\n",
    "\n",
    "##### **Strategies to improve graph retrieval system:**\n",
    "- ***1. Filtering Graph Schema***\n",
    "- ***2. Validating the Cypher Query***\n",
    "- ***3. Few-shot prompting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ce71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================\n",
    "# 1. Filtering \n",
    "#=====================\n",
    "\n",
    "# Import the GraphCypherQAChain for querying Neo4j using natural language\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain  \n",
    "\n",
    "# Initialize the LLM (LLaMA 3.1 from Groq)\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7, max_tokens=100)\n",
    "\n",
    "# Create a QA chain with filtering enabled\n",
    "# exclude_types=[\"Concept\"] will ignore 'Concept' nodes in generated Cypher queries\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    exclude_types=[\"Concept\"], \n",
    "    verbose=True  # Show generated Cypher and execution logs\n",
    ")\n",
    "\n",
    "# Print the current graph schema (useful to verify node types being filtered)\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049aefb",
   "metadata": {},
   "source": [
    "```\n",
    "Node properties: \n",
    "Document {title: STRING, id: STRING, text: STRING, summary: STRING, source: STRING} \n",
    "Organization {id: STRING} \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================\n",
    "# 2. Validating the Cypher query\n",
    "#================================\n",
    "\n",
    "# Create a QA chain with Cypher validation enabled\n",
    "# validate_cypher=True ensures the generated query:\n",
    "# - follows schema rules\n",
    "# - uses correct relationship directions\n",
    "# - avoids invalid node/edge types\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    validate_cypher=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90089ee7",
   "metadata": {},
   "source": [
    "1. Detects nodes and relationships\n",
    "2. Determines the directions of the relationship\n",
    "3. Checks the graph schema\n",
    "4. Update the direction of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d22091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================\n",
    "# 3. Few-shot prompting\n",
    "#======================\n",
    "\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate  \n",
    "\n",
    "# Define a few-shot set: sample questions and their correct Cypher queries\n",
    "examples = [ \n",
    "    { \n",
    "        \"question\": \"How many notable large language models are mentioned in the article?\", \n",
    "        \"query\": \"MATCH (m:Concept {id: 'Large Language Model'}) RETURN count(DISTINCT m)\", \n",
    "    }, \n",
    "    { \n",
    "        \"question\": \"Which companies or organizations have developed the large language models mentioned?\", \n",
    "        \"query\": \"MATCH (o:Organization)-[:DEVELOPS]->(m:Concept {id: 'Large Language Model'}) RETURN DISTINCT o.id\", \n",
    "    }, \n",
    "    { \n",
    "        \"question\": \"What is the largest model size mentioned in the article, in terms of number of parameters?\", \n",
    "        \"query\": \"MATCH (m:Concept {id: 'Large Language Model'}) RETURN max(m.parameters) AS largest_model\", \n",
    "    }, \n",
    "]\n",
    "\n",
    "# Format each example into a consistent prompt structure\n",
    "example_prompt = PromptTemplate.from_template(\"User input: {question}\\nCypher query: {query}\")  \n",
    "\n",
    "# Construct the few-shot Cypher prompt with schema injection\n",
    "cypher_prompt = FewShotPromptTemplate( \n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt, \n",
    "    prefix=\"\"\"You are a Neo4j expert. Given an input question, create a syntactically correct\n",
    "            Cypher query to run.\\n\\nHere is the schema information\\n{schema}.\\n\\n\n",
    "            Below are a number of examples of questions and their corresponding Cypher queries.\"\"\", \n",
    "    suffix=\"User input: {question}\\nCypher query: \", \n",
    "    input_variables=[\"question\"], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================\n",
    "# Adding Few-shot Prompt to Chain\n",
    "#=================================\n",
    "\n",
    "# Create a QA chain with:\n",
    "# - Few-shot Cypher prompt for better query generation\n",
    "# - Cypher validation to ensure correctness\n",
    "# - Verbose mode to log internal steps\n",
    "chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    verbose=True,\n",
    "    validate_cypher=True \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Developing_LLMs_Applications_with_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
