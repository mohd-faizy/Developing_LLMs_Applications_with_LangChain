{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bab3a51",
   "metadata": {},
   "source": [
    "# ü§ñ**RAG Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20157b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Warning Message Procedure Cruise Control Fault Indicates that the cruise control system has detected a fault. Please consult an MG Authorised Repairer as soon as possible. Active Speed Limiter Fault Indicates that the active speed limit system has detected a fault. Contact an MG Authorised Repairer' metadata={'source': 'data/mg-zs-warning-messages.html'}\n",
      "The \"Gasoline Particular Filter Full\" warning indicates that the gasoline particular filter is full and needs to be replaced. You should consult an MG Authorised Repairer as soon as possible to have it replaced. They can properly diagnose and fix the issue.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Imports: Core LangChain and Dependencies\n",
    "# ========================================\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ========================\n",
    "# Environment Preparation\n",
    "# ========================\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# =============================\n",
    "# Step 1: Load HTML Document(s)\n",
    "# =============================\n",
    "\n",
    "loader = UnstructuredHTMLLoader(file_path=\"data/mg-zs-warning-messages.html\")\n",
    "car_docs = loader.load()\n",
    "\n",
    "# Optional: Preview first loaded document\n",
    "# print(car_docs[0])\n",
    "\n",
    "# =============================\n",
    "# Step 2: Split HTML into Chunks\n",
    "# =============================\n",
    "\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(car_docs)\n",
    "\n",
    "# Optional: Preview first chunk\n",
    "print(docs[0])\n",
    "\n",
    "# =============================\n",
    "# Step 3: Generate Embeddings\n",
    "# =============================\n",
    "\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Store embeddings in a persistent Chroma vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Configure the Retriever\n",
    "# ===============================\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 5: Define the Prompt\n",
    "# ==========================\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 6: Initialize the LLM\n",
    "# ==========================\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# Step 7: Create the Retrieval QA Chain\n",
    "# =====================================\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# Step 8: Query the System with a Prompt\n",
    "# ======================================\n",
    "\n",
    "question = \"The Gasoline Particular Filter Full warning has appeared. What does this mean and what should I do about it?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# Print the final answer\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cb14b0",
   "metadata": {},
   "source": [
    "# ‚≠ï**Components used in your CODE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48faf2d",
   "metadata": {},
   "source": [
    "## üîÅ `RunnablePassthrough` (from `langchain_core.runnables`)\n",
    "\n",
    "**Purpose:**\n",
    "This is a special component in LangChain that simply **forwards its input as-is** to the next step in the chain. It acts like a placeholder that does nothing except pass the value along.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "runnable = RunnablePassthrough()\n",
    "output = runnable.invoke(\"input text\")  # returns \"input text\"\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "In the RAG pipeline, we often need to feed the user‚Äôs question directly into the final template. But when using a dictionary to pass multiple inputs to a chain (e.g., `{\"context\": ..., \"question\": ...}`), we must wrap each source properly. Here, `RunnablePassthrough()` is used to make the `question` available downstream without modification.\n",
    "\n",
    "---\n",
    "\n",
    "## üìû `.invoke()` Method\n",
    "\n",
    "**Purpose:**\n",
    "The `invoke()` method is how you **execute a LangChain chain** with a given input and get the final result. It‚Äôs part of the `Runnable` interface in LangChain.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "response = chain.invoke(input_data)\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "response = rag_chain.invoke(\"What is the capital of France?\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "`.invoke()` executes your entire RAG pipeline, from retrieval ‚Üí prompt construction ‚Üí LLM response, and returns the output.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© `PromptTemplate` (from `langchain.prompts`)\n",
    "\n",
    "**Purpose:**\n",
    "`PromptTemplate` helps you **define and format a prompt** dynamically by inserting variables (like user input or context) into a fixed template.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "    Use the context below to answer the question:\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "formatted_prompt = prompt.format(question=\"What is AI?\", context=\"AI stands for Artificial Intelligence...\")\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "It provides clean separation between your prompt logic and runtime inputs. This allows dynamic insertion of question and retrieved context.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç `as_retriever()` (from `Chroma` Vector Store)\n",
    "\n",
    "**Purpose:**\n",
    "The `as_retriever()` method transforms a vector store (like Chroma) into a **retriever object**, which can fetch similar documents given a user query.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",       # or 'mmr'\n",
    "    search_kwargs={\"k\": 3}          # fetch top 3 matches\n",
    ")\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "The RAG system needs a way to fetch relevant context based on user queries. This method provides that interface.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† `ChatGroq` (LLM Wrapper)\n",
    "\n",
    "**Purpose:**\n",
    "`ChatGroq` is an LLM wrapper that lets you interface with Groq-hosted large language models (e.g., LLaMA 3, Mixtral) through LangChain.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "This is the final step in the pipeline. After the prompt is ready, the LLM uses it to generate the answer.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± `RecursiveCharacterTextSplitter`\n",
    "\n",
    "**Purpose:**\n",
    "Splits long documents into manageable text chunks using recursive heuristics and custom separators.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "LLMs have context length limits. This helps fit content into that limit while preserving semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö `GoogleGenerativeAIEmbeddings`\n",
    "\n",
    "**Purpose:**\n",
    "This creates numerical representations (vectors) of text using Google‚Äôs `embedding-001` model. These vectors can be stored in a vector database for similarity search.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "Vector embeddings allow semantic search ‚Äî you can find similar content even if the wording is different.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† `Chroma.from_documents`\n",
    "\n",
    "**Purpose:**\n",
    "Indexes your documents by converting them into vectors and storing them persistently for fast retrieval.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,  # List of Document objects\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=os.getcwd()  # optional: store in current directory\n",
    ")\n",
    "```\n",
    "\n",
    "**Why it's used:**\n",
    "It builds the backend database that powers the semantic retrieval step.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó RAG Chain Composition\n",
    "\n",
    "**This part:**\n",
    "\n",
    "```python\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "This composes a chain of operations in LangChain using the pipe (`|`) operator:\n",
    "\n",
    "1. `{\"context\": retriever, \"question\": RunnablePassthrough()}`:\n",
    "   Combines two inputs: retrieved context + user question\n",
    "\n",
    "2. `| prompt_template`:\n",
    "   Fills the prompt template with the context and question\n",
    "\n",
    "3. `| llm`:\n",
    "   Sends the final prompt to the LLM and gets the response\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Developing_LLMs_Applications_with_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
