{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bab3a51",
   "metadata": {},
   "source": [
    "# ðŸ¤–**RAG Chatbot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20157b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Warning Message Procedure Cruise Control Fault Indicates that the cruise control system has detected a fault. Please consult an MG Authorised Repairer as soon as possible. Active Speed Limiter Fault Indicates that the active speed limit system has detected a fault. Contact an MG Authorised Repairer' metadata={'source': 'data/mg-zs-warning-messages.html'}\n",
      "The \"Gasoline Particular Filter Full\" warning indicates that the gasoline particular filter is full and needs to be replaced. You should consult an MG Authorised Repairer as soon as possible to have it replaced. They can properly diagnose and fix the issue.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Imports: Core LangChain and Dependencies\n",
    "# ========================================\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma as ChromaCommunity\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ========================\n",
    "# Environment Preparation\n",
    "# ========================\n",
    "\n",
    "load_dotenv()  # Load environment variables from a .env file\n",
    "\n",
    "# =============================\n",
    "# Step 1: Load HTML Document(s)\n",
    "# =============================\n",
    "\n",
    "loader = UnstructuredHTMLLoader(file_path=\"data/mg-zs-warning-messages.html\")\n",
    "car_docs = loader.load()\n",
    "\n",
    "# Optional: Preview first loaded document\n",
    "# print(car_docs[0])\n",
    "\n",
    "# =============================\n",
    "# Step 2: Split HTML into Chunks\n",
    "# =============================\n",
    "\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(car_docs)\n",
    "\n",
    "# Optional: Preview first chunk\n",
    "print(docs[0])\n",
    "\n",
    "# =============================\n",
    "# Step 3: Generate Embeddings\n",
    "# =============================\n",
    "\n",
    "embedding_function = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Store embeddings in a persistent Chroma vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=os.getcwd()\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Configure the Retriever\n",
    "# ===============================\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 5: Define the Prompt\n",
    "# ==========================\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# Step 6: Initialize the LLM\n",
    "# ==========================\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# =====================================\n",
    "# Step 7: Create the Retrieval QA Chain\n",
    "# =====================================\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# Step 8: Query the System with a Prompt\n",
    "# ======================================\n",
    "\n",
    "question = \"The Gasoline Particular Filter Full warning has appeared. What does this mean and what should I do about it?\"\n",
    "response = rag_chain.invoke(question)\n",
    "\n",
    "# Print the final answer\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Developing_LLMs_Applications_with_LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
