{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60324084",
   "metadata": {},
   "source": [
    "# â­**Designing Agentic Systems with LangChain**\n",
    "\n",
    "This notebook demonstrates how to integrate and dynamically route multiple tools in LangGraph, enabling flexible and memory-aware educational chatbot workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f46d47",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [***1. Introduction to Multiple Tools***](#1-introduction-to-multiple-tools)\n",
    "- [***2. Custom Tools: Palindrome and Historical Dates***](#2-custom-tools-palindrome-and-historical-dates)\n",
    "- [***3. Binding Multiple Tools***](#3-binding-multiple-tools)\n",
    "- [***4. Building a Flexible Workflow***](#4-building-a-flexible-workflow)\n",
    "- [***5. Streaming and Memory Integration***](#5-streaming-and-memory-integration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7831d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Required modules\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c13c22",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduction to Multiple Tools\n",
    "\n",
    "LangGraph allows agents to use multiple tools and determine the correct one based on user input.\n",
    "\n",
    "- Automate tool selection per query\n",
    "- Support multi-functional bots (e.g., palindrome checkers and historical lookup bots)\n",
    "- Enable natural language control over which function is executed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f0988",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Custom Tools: Palindrome and Historical Dates\n",
    "\n",
    "Define custom tools using the `@tool` decorator for clarity and control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f15774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool: Historical date lookup using LLM\n",
    "@tool\n",
    "def date_checker(date: str) -> str:\n",
    "    \"\"\"Provide a list of important historical events for a given date.\"\"\"\n",
    "    try:\n",
    "        answer = llm.invoke(f\"List important historical events that occurred on {date}.\")\n",
    "        return answer.content\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving events: {str(e)}\"\n",
    "\n",
    "# Tool: Palindrome check using Python logic\n",
    "@tool\n",
    "def check_palindrome(text: str):\n",
    "    \"\"\"Check if a word or phrase is a palindrome.\"\"\"\n",
    "    cleaned = ''.join(char.lower() for char in text if char.isalnum())\n",
    "    if cleaned == cleaned[::-1]:\n",
    "        return f\"The phrase or word '{text}' is a palindrome.\"\n",
    "    else:\n",
    "        return f\"The phrase or word '{text}' is not a palindrome.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65eec7",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Binding Multiple Tools\n",
    "\n",
    "Bind tools to a ToolNode and connect to the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa006acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"OPENAI_API_KEY\")\n",
    "\n",
    "# Bundle all tools\n",
    "tools = [date_checker, check_palindrome]\n",
    "tool_node = ToolNode(tools)\n",
    "model_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a48d5c",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Building a Flexible Workflow\n",
    "\n",
    "Define graph logic to handle both LLM-only responses and tool-based outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping condition: route based on tool call presence\n",
    "def should_continue(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    return \"tools\" if last_message.tool_calls else END\n",
    "\n",
    "# Main model function: decides between LLM response or tool return\n",
    "def call_model(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        return {\"messages\": [AIMessage(content=last_message.tool_calls[0][\"response\"])]}\n",
    "    return {\"messages\": [model_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build the full graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"chatbot\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_conditional_edges(\"chatbot\", should_continue, [\"tools\", END])\n",
    "workflow.add_edge(\"tools\", \"chatbot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba44e5a",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Streaming and Memory Integration\n",
    "\n",
    "Enable memory for multi-turn conversations and test with dynamic tool usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory and compile the app\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62400cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stream responses\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "def multi_tool_output(query):\n",
    "    inputs = {\"messages\": [HumanMessage(content=query)]}\n",
    "    for msg, metadata in app.stream(inputs, config, stream_mode=\"messages\"):\n",
    "        if msg.content and not isinstance(msg, HumanMessage):\n",
    "            print(msg.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Example tool usage\n",
    "multi_tool_output(\"Is `Stella won no wallets` a palindrome?\")\n",
    "multi_tool_output(\"What happened on April 12th, 1955?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up multi-turn conversation\n",
    "def user_agent_multiturn(queries):\n",
    "    for query in queries:\n",
    "        print(f\"User: {query}\")\n",
    "        print(\"Agent: \" + \"\".join(\n",
    "            msg.content for msg, metadata in app.stream(\n",
    "                {\"messages\": [HumanMessage(content=query)]},\n",
    "                config,\n",
    "                stream_mode=\"messages\"\n",
    "            ) if msg.content and not isinstance(msg, HumanMessage)\n",
    "        ) + \"\\n\")\n",
    "\n",
    "queries = [\n",
    "    \"What happened on the 12 April 1961?\",\n",
    "    \"What about 10 December 1948?\",\n",
    "    \"Is `Mr. Owl ate my metal worm?` a palindrome?\",\n",
    "    \"What about 'palladium stadium?'\"\n",
    "]\n",
    "user_agent_multiturn(queries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
